import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import deque
import random
from typing import List, Optional
from src.core.data_structures import MCPMetadata, Task, Experience

class DynamicQNetwork(nn.Module):
    """åŠ¨æ€Qç½‘ç»œï¼ˆæ¥å—action embeddingï¼‰"""
    
    def __init__(self, state_dim: int, action_emb_dim: int, hidden_dim: int = 256):
        super().__init__()
        input_dim = state_dim + action_emb_dim
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        print(f"[DQN] ç½‘ç»œè¾“å…¥ç»´åº¦: {input_dim} (state:{state_dim} + action:{action_emb_dim})")
    
    def forward(self, state, action_embedding):
        """
        Args:
            state: [batch_size, state_dim]
            action_embedding: [batch_size, action_emb_dim]
        Returns:
            q_values: [batch_size, 1]
        """
        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if len(state.shape) == 1:
            state = state.unsqueeze(0)
        if len(action_embedding.shape) == 1:
            action_embedding = action_embedding.unsqueeze(0)
        
        combined = torch.cat([state, action_embedding], dim=-1)
        return self.network(combined)

class DynamicDQNAgent:
    """åŠ¨æ€DQNæ™ºèƒ½ä½“ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼šå¤„ç†åŠ¨æ€åŠ¨ä½œç©ºé—´ï¼‰"""
    
    def __init__(self, config: dict, retriever):
        self.config = config
        self.retriever = retriever
        
        # ç½‘ç»œ
        self.q_network = DynamicQNetwork(
            config['state_dim'],
            config['action_emb_dim'],
            config['hidden_dim']
        )
        self.target_network = DynamicQNetwork(
            config['state_dim'],
            config['action_emb_dim'],
            config['hidden_dim']
        )
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config['learning_rate'])
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = deque(maxlen=config['replay_buffer_size'])
        
        # æ¢ç´¢ç­–ç•¥
        self.epsilon = config['epsilon_start']
        self.epsilon_end = config['epsilon_end']
        self.epsilon_decay = config['epsilon_decay']
        
        # è®­ç»ƒå‚æ•°
        self.gamma = config['gamma']
        self.batch_size = config['batch_size']
        
        # ç»Ÿè®¡
        self.training_steps = 0
        self.episode_rewards = []
    
    def get_state(self, task: Task, execution_history: List[dict]) -> np.ndarray:
        """
        æ„å»ºçŠ¶æ€è¡¨ç¤º
        
        Args:
            task: å½“å‰ä»»åŠ¡
            execution_history: æœ€è¿‘çš„æ‰§è¡Œå†å²
        
        Returns:
            stateå‘é‡ [state_dim]
        """
        # ä»»åŠ¡åµŒå…¥
        task_emb = self.retriever.encode_task(task)
        
        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if len(task_emb.shape) == 0:
            task_emb = np.array([task_emb])
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        if self.training_steps == 0:
            print(f"[DQN] Stateç»´åº¦: {task_emb.shape}")
        
        return task_emb
    
    def encode_action(self, mcp: MCPMetadata) -> np.ndarray:
        """ç¼–ç åŠ¨ä½œï¼ˆMCPï¼‰"""
        action_emb = self.retriever.encode_mcp(mcp)
        
        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if len(action_emb.shape) == 0:
            action_emb = np.array([action_emb])
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        if self.training_steps == 0:
            print(f"[DQN] Actionç»´åº¦: {action_emb.shape}")
        
        return action_emb
    
    def select_action(self, state: np.ndarray, candidates: List[MCPMetadata]) -> MCPMetadata:
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆMCPï¼‰
        
        æ ¸å¿ƒåˆ›æ–°ï¼šå³ä½¿MCP Boxå¢é•¿ï¼Œä¹Ÿèƒ½å¤„ç†
        """
        if not candidates:
            raise ValueError("å€™é€‰MCPåˆ—è¡¨ä¸ºç©º")
        
        # Epsilon-greedyæ¢ç´¢
        if random.random() < self.epsilon:
            return random.choice(candidates)
        
        # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„MCP
        self.q_network.eval()
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state)
            if len(state_tensor.shape) == 1:
                state_tensor = state_tensor.unsqueeze(0)
            
            q_values = []
            for mcp in candidates:
                action_emb = self.encode_action(mcp)
                action_tensor = torch.FloatTensor(action_emb)
                if len(action_tensor.shape) == 1:
                    action_tensor = action_tensor.unsqueeze(0)
                
                # æ‰“å°ç»´åº¦è°ƒè¯•ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
                if len(q_values) == 0 and self.training_steps == 0:
                    print(f"[DQN] state_tensor: {state_tensor.shape}, action_tensor: {action_tensor.shape}")
                
                q = self.q_network(state_tensor, action_tensor)
                q_values.append(q.item())
        
        best_idx = np.argmax(q_values)
        return candidates[best_idx]
    
    def store_experience(self, state, action_mcp, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        action_emb = self.encode_action(action_mcp)
        experience = (state, action_emb, reward, next_state, done)
        self.replay_buffer.append(experience)
    
    def train_step(self) -> Optional[float]:
        """è®­ç»ƒä¸€æ­¥"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·batch
        batch = random.sample(self.replay_buffer, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # è½¬æ¢ä¸ºtensorå¹¶ç¡®ä¿ç»´åº¦æ­£ç¡®
        states = torch.FloatTensor(np.array(states))
        actions = torch.FloatTensor(np.array(actions))
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(dones)
        
        # ç¡®ä¿batchç»´åº¦
        if len(states.shape) == 1:
            states = states.unsqueeze(0)
        if len(actions.shape) == 1:
            actions = actions.unsqueeze(0)
        if len(next_states.shape) == 1:
            next_states = next_states.unsqueeze(0)
        
        # å½“å‰Qå€¼
        self.q_network.train()
        current_q = self.q_network(states, actions).squeeze()
        
        # ç›®æ ‡Qå€¼ï¼ˆç®€åŒ–ï¼šä½¿ç”¨ç›¸åŒaction embeddingï¼‰
        with torch.no_grad():
            next_q = self.target_network(next_states, actions).squeeze()
            target_q = rewards + (1 - dones) * self.gamma * next_q
        
        # æŸå¤±
        loss = nn.MSELoss()(current_q, target_q)
        
        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª
        self.optimizer.step()
        
        # æ›´æ–°epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        
        # å®šæœŸæ›´æ–°targetç½‘ç»œ
        self.training_steps += 1
        if self.training_steps % 100 == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        return loss.item()
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'training_steps': self.training_steps
        }, path)
        print(f"ğŸ’¾ DQNæ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']
        self.training_steps = checkpoint['training_steps']
        print(f"ğŸ“‚ DQNæ¨¡å‹å·²ä» {path} åŠ è½½")